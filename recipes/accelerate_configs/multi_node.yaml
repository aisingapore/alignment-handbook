compute_environment: LOCAL_MACHINE # LOCAL_MACHINE, AMAZON_SAGEMAKER
debug: true # Print out torch.distributed stack trace on failure

# Resource selection
mixed_precision: bf16
num_machines: null
num_processes: null
use_cpu: false

# Training paradigm
distributed_type: FSDP
fsdp_config:
  fsdp_auto_wrap_policy: TRANSFORMER_BASED_WRAP
  fsdp_backward_prefetch_policy: BACKWARD_PRE
  fsdp_forward_prefetch: false
  fsdp_cpu_ram_efficient_loading: true
  fsdp_offload_params: false
  fsdp_sharding_strategy: FULL_SHARD
  fsdp_state_dict_type: SHARDED_STATE_DICT
  fsdp_sync_module_states: true
  fsdp_transformer_layer_cls_to_wrap: GemmaDecoderLayer
  fsdp_use_orig_params: true

# Distributed config
gpu_ids: all
machine_rank: null
main_process_ip: null
main_process_port: null
rdzv_backend: c10d
same_network: true

# TPU (Potentially useless when not using TPU)
downcast_bf16: 'no'
main_training_function: main
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
